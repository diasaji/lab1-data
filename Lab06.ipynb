{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "648344fe-4eba-456c-b3af-7ea2f11ef6ac",
      "metadata": {
        "id": "648344fe-4eba-456c-b3af-7ea2f11ef6ac"
      },
      "source": [
        "# Lab 06 Reccurrent Neural Networks\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2026/blob/main/Lab06.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "The purpose of this lab is to see the application of RNNs in practice, to re-enforce what you have seen in the lectures. We will look at the implementation of RNNs in parcatice, and a simplified method of processing textual data to create a basic Natural Language Processing model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9e862a7-3d19-494d-a426-04fc077a7768",
      "metadata": {
        "id": "f9e862a7-3d19-494d-a426-04fc077a7768"
      },
      "source": [
        "## Task: Name Classification\n",
        "\n",
        "Based on the tutorial from: https://docs.pytorch.org/tutorials/interme\n",
        "\n",
        "We are given a dataset of surnames, and a corresponding label indicating which of 18 possible languages of origin the name is written in.\n",
        "\n",
        "We will define a charcter-level RNN that will attempt to classify the origin language, based on the spelling of the name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "19205dbe-23d3-4aca-9e9d-7a22b0c4ef69",
      "metadata": {
        "id": "19205dbe-23d3-4aca-9e9d-7a22b0c4ef69",
        "outputId": "47c472bb-eea5-49a8-b5a9-b74b299323ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device = cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "torch.set_default_device(device)\n",
        "print(f\"Using device = {torch.get_default_device()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6903a8-0404-4487-aca8-b1a7208a05bb",
      "metadata": {
        "id": "0b6903a8-0404-4487-aca8-b1a7208a05bb"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "99605a9e-81ea-49df-8c80-f0f9518753a9",
      "metadata": {
        "tags": [],
        "id": "99605a9e-81ea-49df-8c80-f0f9518753a9",
        "outputId": "49741ba5-73c4-4782-bbdf-f1e16be9117e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-26 15:12:20--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.160.10.36, 18.160.10.28, 18.160.10.76, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.160.10.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2026-02-26 15:12:21 (35.0 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a6b3af2a-a4c0-4f41-b2c1-8478e7056060",
      "metadata": {
        "tags": [],
        "id": "a6b3af2a-a4c0-4f41-b2c1-8478e7056060",
        "outputId": "038615e8-0e4f-420e-b610-415c82207c3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d8eade-c744-480f-b46c-af0bbd2edce4",
      "metadata": {
        "id": "93d8eade-c744-480f-b46c-af0bbd2edce4"
      },
      "source": [
        "First;y, we will handle special characters. Some languages use a broad range of accents with letters, that we may wish to incorporate in more complex models. For our simple RNN model, we will constrain ourselves to:\n",
        "\n",
        "1. ASCII letters (accentless) A-Z and a-z (uppercase and lowercase)\n",
        "2. The characters: (space) . , ; ' (single quote) and _\n",
        "\n",
        "This gives a total of 58 possible characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "00899b35-852c-45ce-ae63-7b1fee459c14",
      "metadata": {
        "tags": [],
        "id": "00899b35-852c-45ce-ae63-7b1fee459c14"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import unicodedata\n",
        "\n",
        "# We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
        "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
        "n_letters = len(allowed_characters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in allowed_characters\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "52a40520-91a5-4427-bb05-14c4ce066608",
      "metadata": {
        "tags": [],
        "id": "52a40520-91a5-4427-bb05-14c4ce066608",
        "outputId": "09d340bd-b337-487b-b7ce-598effec4173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converting 'Ślusàrski' to Slusarski\n"
          ]
        }
      ],
      "source": [
        "print (f\"converting 'Ślusàrski' to {unicodeToAscii('Ślusàrski')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c4d47a-e0cc-447f-85ae-ab533ebc1ddf",
      "metadata": {
        "id": "08c4d47a-e0cc-447f-85ae-ab533ebc1ddf"
      },
      "source": [
        "### One-Hot Encoding\n",
        "\n",
        "We now have names that consist of only accentless A-Z and a-z characters.\n",
        "\n",
        "We still need to convert this into a numeric Tensor form, that we can then provide as input to an RNN model.\n",
        "\n",
        "To do so, we will represent each of our 58 possible characters by a \"one-hot\" vector, as we mentioned in previous labs.\n",
        "\n",
        "Consider a vector of 58 values. Imagine that each index corresponds to possible valid character.\n",
        "\n",
        "We can represent any of our 58 possible values by a vector of size 58, with 57 zero values and a singular 1 value -- the position of this 1 value defines what character this vector represents.\n",
        "\n",
        "E.g.\n",
        "\n",
        "[1,0,0,0,0,0,0....,0,0,0] Could represent the character A\n",
        "\n",
        "[0,1,0,0,0,0,0....,0,0,0] Could represent the character B\n",
        "\n",
        "[0,0,1,0,0,0,0....,0,0,0] Could represent the character C\n",
        "\n",
        "...\n",
        "\n",
        "[0,0,0,0,0,0,0....,0,0,1] Could represent the character _\n",
        "\n",
        "\n",
        "A given name, say \"Smith\" is can now be represented by a (5x58) matrix -- the number of rows corresponds to the number of letters in the name, the number of columns the size of the one-hot vectors that represent the letters.\n",
        "\n",
        "In practice, PyTorch also requires a batch dimension. Potentially unintuitively, the batch dimension is between the number of characters, and the one-hot vector. I.e a batch of two names, each 5 characters long, will be a Tensor with shape (5x2x58).\n",
        "\n",
        "**Why?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "086b113b-7d7e-41c8-b558-f43732df9a13",
      "metadata": {
        "tags": [],
        "id": "086b113b-7d7e-41c8-b558-f43732df9a13"
      },
      "outputs": [],
      "source": [
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    # return our out-of-vocabulary character if we encounter a letter unknown to our model\n",
        "    if letter not in allowed_characters:\n",
        "        return allowed_characters.find(\"_\")\n",
        "    else:\n",
        "        return allowed_characters.find(letter)\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "535ee6b3-20d6-451a-aeeb-fdf67cd4585d",
      "metadata": {
        "tags": [],
        "id": "535ee6b3-20d6-451a-aeeb-fdf67cd4585d",
        "outputId": "80f6327c-d1c1-4ec1-d6df-15ffbbf23a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The letter 'a' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]]])\n",
            "The name 'Ahn' becomes tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]]])\n"
          ]
        }
      ],
      "source": [
        "print (f\"The letter 'a' becomes {lineToTensor('a')}\") #notice that the first position in the tensor = 1\n",
        "print (f\"The name 'Ahn' becomes {lineToTensor('Ahn')}\") #notice 'A' sets the 27th index to 1~"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "855db93a-001e-4a17-a96b-72e6716fea63",
      "metadata": {
        "id": "855db93a-001e-4a17-a96b-72e6716fea63"
      },
      "source": [
        "We can take our dataset of names, and crate Datatsets as usual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "aea9480e-9f76-4f51-a235-2a9e7e35ee93",
      "metadata": {
        "tags": [],
        "id": "aea9480e-9f76-4f51-a235-2a9e7e35ee93"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NamesDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir #for provenance of the dataset\n",
        "        self.load_time = time.localtime #for provenance of the dataset\n",
        "        labels_set = set() #set of all classes\n",
        "\n",
        "        self.data = []\n",
        "        self.data_tensors = []\n",
        "        self.labels = []\n",
        "        self.labels_tensors = []\n",
        "\n",
        "        #read all the ``.txt`` files in the specified directory\n",
        "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
        "        for filename in text_files:\n",
        "            label = os.path.splitext(os.path.basename(filename))[0]\n",
        "            labels_set.add(label)\n",
        "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "            for name in lines:\n",
        "                self.data.append(name)\n",
        "                self.data_tensors.append(lineToTensor(name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        #Cache the tensor representation of the labels\n",
        "        self.labels_uniq = list(labels_set)\n",
        "        for idx in range(len(self.labels)):\n",
        "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
        "            self.labels_tensors.append(temp_tensor)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_item = self.data[idx]\n",
        "        data_label = self.labels[idx]\n",
        "        data_tensor = self.data_tensors[idx]\n",
        "        label_tensor = self.labels_tensors[idx]\n",
        "\n",
        "        return label_tensor, data_tensor, data_label, data_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c9a6f4be-7963-4a23-9d2f-0d08122495bd",
      "metadata": {
        "tags": [],
        "id": "c9a6f4be-7963-4a23-9d2f-0d08122495bd",
        "outputId": "0dad6204-dbd0-42e7-cb16-05e1fd8cd289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 20074 items of data\n",
            "example = (tensor([7]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]]]), 'German', 'Abbing')\n"
          ]
        }
      ],
      "source": [
        "alldata = NamesDataset(\"data/names\")\n",
        "print(f\"loaded {len(alldata)} items of data\")\n",
        "print(f\"example = {alldata[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27108daa-fe76-4302-babd-8a9583a33088",
      "metadata": {
        "tags": [],
        "id": "27108daa-fe76-4302-babd-8a9583a33088",
        "outputId": "554490e8-a2c4-402c-c8d1-7887e04640ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train examples = 17063, validation examples = 3011\n"
          ]
        }
      ],
      "source": [
        "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
        "\n",
        "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d7335b-2d99-4d53-bbb0-1c30061e7848",
      "metadata": {
        "id": "c4d7335b-2d99-4d53-bbb0-1c30061e7848"
      },
      "source": [
        "# Defining Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68624bed-2928-46c6-983a-6aee03358d29",
      "metadata": {
        "id": "68624bed-2928-46c6-983a-6aee03358d29"
      },
      "source": [
        "## Linear Model\n",
        "\n",
        "If you were to try to apply a fully connected model to this data, what are the major disadvantages?\n",
        "\n",
        "Hint: Look at a few instances of: `train_set[x][1].shape`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d9b0e812-9c4e-49f3-8d2d-6c19cf88fe08",
      "metadata": {
        "id": "d9b0e812-9c4e-49f3-8d2d-6c19cf88fe08",
        "outputId": "1608a05a-5ddd-4c4a-b866-35810e925e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([11, 1, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_set[4][1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a894f13a-7fd6-41d0-9272-b278827b9592",
      "metadata": {
        "id": "a894f13a-7fd6-41d0-9272-b278827b9592"
      },
      "source": [
        "## RNN Model\n",
        "\n",
        "We are attempting to implement a classificaiton model, in a similar vein as Sentiment Classfication, as you see in Slide 34 of Lecture 12. Refer to this diagram.\n",
        "\n",
        "We willl use PyTorch to implement this directly.\n",
        "\n",
        "Bear in mind that we are predicting which of 18 possible langaues our inputted name coresponds to. **What sould our output shape therefore be?**\n",
        "\n",
        "The code to implement this RNN is trivial:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_letters"
      ],
      "metadata": {
        "id": "VhKMUANimhiD",
        "outputId": "a1bd2ba9-7e36-4dcb-bbff-2932167af4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VhKMUANimhiD",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f1adba89-6664-4a4d-a02f-65939455090b",
      "metadata": {
        "tags": [],
        "id": "f1adba89-6664-4a4d-a02f-65939455090b"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, line_tensor):\n",
        "        rnn_out, hidden = self.rnn(line_tensor)\n",
        "        output = self.h2o(hidden[0])\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5603e00e-6f69-482e-a7d3-7ef28e451ba7",
      "metadata": {
        "id": "5603e00e-6f69-482e-a7d3-7ef28e451ba7"
      },
      "source": [
        "It is worthwhile taking a brief moment to look at how PyTorch implements this RNN, and how it compares to what you have seen in the lectures:\n",
        "\n",
        "\n",
        "\n",
        "```python3\n",
        "# Efficient implementation equivalent to the following with bidirectional=False\n",
        "rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
        "params = dict(rnn.named_parameters())\n",
        "def forward(x, hx=None, batch_first=False):\n",
        "    if batch_first:\n",
        "        x = x.transpose(0, 1)\n",
        "    seq_len, batch_size, _ = x.size()\n",
        "    if hx is None:\n",
        "        hx = torch.zeros(rnn.num_layers, batch_size, rnn.hidden_size)\n",
        "    h_t_minus_1 = hx.clone()\n",
        "    h_t = hx.clone()\n",
        "    output = []\n",
        "    for t in range(seq_len):\n",
        "        for layer in range(rnn.num_layers):\n",
        "            input_t = x[t] if layer == 0 else h_t[layer - 1]\n",
        "            h_t[layer] = torch.tanh(\n",
        "                input_t @ params[f\"weight_ih_l{layer}\"].T\n",
        "                + h_t_minus_1[layer] @ params[f\"weight_hh_l{layer}\"].T\n",
        "                + params[f\"bias_hh_l{layer}\"]\n",
        "                + params[f\"bias_ih_l{layer}\"]\n",
        "            )\n",
        "        output.append(h_t[-1].clone())\n",
        "        h_t_minus_1 = h_t.clone()\n",
        "    output = torch.stack(output)\n",
        "    if batch_first:\n",
        "        output = output.transpose(0, 1)\n",
        "    return output, h_t\n",
        "```\n",
        "    \n",
        "https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e2b1ee24-44e8-403c-80fa-6c6bf90d2a4d",
      "metadata": {
        "tags": [],
        "id": "e2b1ee24-44e8-403c-80fa-6c6bf90d2a4d",
        "outputId": "7f0e0d11-0052-449a-8347-cb336325b350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (rnn): RNN(58, 128)\n",
            "  (h2o): Linear(in_features=128, out_features=18, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "n_hidden = 128\n",
        "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\n",
        "print(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot\n",
        "!pip install cartopy\n",
        "import cartopy"
      ],
      "metadata": {
        "id": "z4mo3u1gnTLJ",
        "outputId": "925ea41b-4b99-4e53-e0dc-e5a98c48f377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "z4mo3u1gnTLJ",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n",
            "E: Package 'libfluidsynth1' has no installation candidate\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 117540 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting libarchive\n",
            "  Downloading libarchive-0.4.7.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nose (from libarchive)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: libarchive\n",
            "  Building wheel for libarchive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libarchive: filename=libarchive-0.4.7-py3-none-any.whl size=31629 sha256=188ed4476f2540ec6de200ca5fe8b758b64271b7c6281c3718ac084040380ccf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/20/ab/f101da7b245b996aa097685ef742243725ea6150f5b3b6d9ed\n",
            "Successfully built libarchive\n",
            "Installing collected packages: nose, libarchive\n",
            "Successfully installed libarchive-0.4.7 nose-1.3.7\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: pyparsing>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from pydot) (3.3.2)\n",
            "Collecting cartopy\n",
            "  Downloading cartopy-0.25.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.12/dist-packages (from cartopy) (3.10.0)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.12/dist-packages (from cartopy) (2.1.2)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from cartopy) (26.0)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.12/dist-packages (from cartopy) (3.0.3)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.12/dist-packages (from cartopy) (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->cartopy) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj>=3.3.1->cartopy) (2026.1.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n",
            "Downloading cartopy-0.25.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cartopy\n",
            "Successfully installed cartopy-0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a2c47126-4a55-4c9f-afa9-540ed8a5b97d",
      "metadata": {
        "tags": [],
        "id": "a2c47126-4a55-4c9f-afa9-540ed8a5b97d",
        "outputId": "1217968e-3629-442b-ee0f-9d6b7ee2fdb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchinfo'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-536/1357982964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchinfo'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "06546ae8-b5ba-43a0-a528-1f361c7b0267",
      "metadata": {
        "tags": [],
        "id": "06546ae8-b5ba-43a0-a528-1f361c7b0267",
        "outputId": "0885ab46-e0a1-4794-8d22-3553a649af43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'summary' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-536/3131130224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'summary' is not defined"
          ]
        }
      ],
      "source": [
        "summary(rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f41955-44e3-4147-b18d-7ce5054a355b",
      "metadata": {
        "id": "42f41955-44e3-4147-b18d-7ce5054a355b"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6d26fe49-4092-482e-8e19-e5535b31f778",
      "metadata": {
        "tags": [],
        "id": "6d26fe49-4092-482e-8e19-e5535b31f778"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n",
        "    \"\"\"\n",
        "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
        "    \"\"\"\n",
        "    # Keep track of losses for plotting\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "    rnn.train()\n",
        "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "    start = time.time()\n",
        "    print(f\"training on data set with n = {len(training_data)}\")\n",
        "\n",
        "    for iter in range(1, n_epoch + 1):\n",
        "        rnn.zero_grad() # clear the gradients\n",
        "\n",
        "        # create some minibatches\n",
        "        # we cannot use dataloaders because each of our names is a different length\n",
        "        batches = list(range(len(training_data)))\n",
        "        random.shuffle(batches)\n",
        "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
        "\n",
        "        for idx, batch in enumerate(batches):\n",
        "            batch_loss = 0\n",
        "            for i in batch: #for each example in this batch\n",
        "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
        "                output = rnn.forward(text_tensor)\n",
        "                loss = criterion(output, label_tensor)\n",
        "                batch_loss += loss\n",
        "\n",
        "            # optimize parameters\n",
        "            batch_loss.backward()\n",
        "\n",
        "            # HERE IS CLIPPING BY NORM!\n",
        "            # In this case, the norm over all parameters is clipped at 3\n",
        "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            current_loss += batch_loss.item() / len(batch)\n",
        "\n",
        "        all_losses.append(current_loss / len(batches) )\n",
        "        if iter % report_every == 0:\n",
        "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
        "        current_loss = 0\n",
        "\n",
        "    return all_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d02ec86-6919-4428-a078-33bc176efe74",
      "metadata": {
        "id": "4d02ec86-6919-4428-a078-33bc176efe74"
      },
      "source": [
        "A good illustration of clipping:\n",
        "\n",
        "<img src='https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/gradient-clipping.png?ssl=1' width='750px'/>\n",
        "\n",
        "**Question: why clip by norm, vs clipping by value?**\n",
        "\n",
        "https://docs.pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "932d3e81-faed-43bd-a5f5-2b7e0ca95a14",
      "metadata": {
        "tags": [],
        "id": "932d3e81-faed-43bd-a5f5-2b7e0ca95a14",
        "outputId": "0695fedd-e3df-4a53-d911-0379e223ff04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on data set with n = 17063\n",
            "5 (19%): \t average batch loss = 0.8839825878707297\n",
            "10 (37%): \t average batch loss = 0.6982163219023745\n",
            "15 (56%): \t average batch loss = 0.5853765573182362\n",
            "20 (74%): \t average batch loss = 0.4999316751663916\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "all_losses = train(rnn, train_set, n_epoch=27, learning_rate=0.15, report_every=5)\n",
        "end = time.time()\n",
        "print(f\"training took {end-start}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac71419-8ac8-447d-8d66-c3239523e7f5",
      "metadata": {
        "id": "aac71419-8ac8-447d-8d66-c3239523e7f5"
      },
      "source": [
        "We can extract named labels from the dataset to better evaluate performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3683d9-f5d2-47d2-bfcf-e95d0851353c",
      "metadata": {
        "tags": [],
        "id": "dc3683d9-f5d2-47d2-bfcf-e95d0851353c"
      },
      "outputs": [],
      "source": [
        "def label_from_output(output, output_labels):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    label_i = top_i[0].item()\n",
        "    return output_labels[label_i], label_i\n",
        "\n",
        "input = lineToTensor('Albert')\n",
        "output = rnn(input) #this is equivalent to ``output = rnn.forward(input)``\n",
        "print(output)\n",
        "print(label_from_output(output, alldata.labels_uniq))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94580a78-1322-4e16-abb9-8b524b2dba44",
      "metadata": {
        "id": "94580a78-1322-4e16-abb9-8b524b2dba44"
      },
      "source": [
        "# Evaluating Results\n",
        "\n",
        "We could use TensorBoard here, but for simplicity, we will quickly evaluate the loss over time in a simple matplotilib plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6c668f-2f3e-47d5-af6b-d95de77714fb",
      "metadata": {
        "tags": [],
        "id": "ec6c668f-2f3e-47d5-af6b-d95de77714fb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "952990df-f549-4d85-927e-789d7b7d54ff",
      "metadata": {
        "id": "952990df-f549-4d85-927e-789d7b7d54ff"
      },
      "source": [
        "Outside of loss, we want to consider a measure of accuracy -- since this is the final metric of actual interest to us. For a multi-class classification task, the best method of doing this is using a heatmap -- this allows us to better understand how our model performs at predicting each indiviudal class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "108c3991-f533-402a-8873-a8c91203698a",
      "metadata": {
        "tags": [],
        "id": "108c3991-f533-402a-8873-a8c91203698a"
      },
      "outputs": [],
      "source": [
        "def evaluate(rnn, testing_data, classes):\n",
        "    confusion = torch.zeros(len(classes), len(classes))\n",
        "\n",
        "    rnn.eval() #set to eval mode\n",
        "    with torch.no_grad(): # do not record the gradients during eval phase\n",
        "        for i in range(len(testing_data)):\n",
        "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
        "            output = rnn(text_tensor)\n",
        "            guess, guess_i = label_from_output(output, classes)\n",
        "            label_i = classes.index(label)\n",
        "            confusion[label_i][guess_i] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(len(classes)):\n",
        "        denom = confusion[i].sum()\n",
        "        if denom > 0:\n",
        "            confusion[i] = confusion[i] / denom\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
        "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
        "\n",
        "    # Force label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    # sphinx_gallery_thumbnail_number = 2\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59eedcc1-fe76-4512-9f84-f2ba25c7b7fc",
      "metadata": {
        "id": "59eedcc1-fe76-4512-9f84-f2ba25c7b7fc"
      },
      "source": [
        "Note that labels on the Y axis are the correct labels, and labels on the X axis are predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48aeb18-2cf8-4ede-a3d0-98112c5867fb",
      "metadata": {
        "id": "a48aeb18-2cf8-4ede-a3d0-98112c5867fb"
      },
      "source": [
        "## Further Exercises and Development\n",
        "\n",
        "1. Tune the model and training hyper-parameters to imporve performace, and reduce the effect of vanishing gradient\n",
        "2. Increasing complexity, adding additional hidden nodes, combining multiple RNNs, or additional Linear layers\n",
        "3.  Moving towards a better RNN implementation, such as LSTM or GRU\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jupyter-student",
      "language": "python",
      "name": "jupyter-eg-kernel-slurm-py-conda-1ja8rhof9"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}